{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = pd.read_csv('Shakespeare_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file['Play'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoded_col = encoder.fit_transform(train_file['Play'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file['Play'] = pd.Series(encoded_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file1 = train_file['PlayerLine'].loc[train_file['ActSceneLine'].isna() == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to combine the rows which have the same `PlayerLinenumber` and where the `ActSceneLine` is of the format 1.1.x. After combining these rows, we split the text in the `PlayerLine` column and give each sentence a row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file1 = train_file.dropna(axis=0)\n",
    "train_file1.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file1.drop(['Dataline', 'Player'], inplace=True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file1 = train_file1.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_cols = train_file1['ActSceneLine'].str.split('.', expand=True)\n",
    "train_file1['Act'] = split_cols[0]\n",
    "train_file1['Scene'] = split_cols[1]\n",
    "train_file1['Line'] = split_cols[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file1.drop('ActSceneLine', inplace=True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_cols = ['PlayerLine']\n",
    "group_cols = ['PlayerLinenumber', 'Act', 'Scene', 'Play']\n",
    "def combine_rows(group):\n",
    "        # Convert each row to a string and join them\n",
    "        rows_combined = '\\n'.join([\n",
    "            ' '.join(map(str, row))\n",
    "            for row in group[other_cols].values\n",
    "        ])\n",
    "        \n",
    "        # Create a new row with group columns and combined string\n",
    "        new_row = pd.Series()\n",
    "        for col in ['PlayerLinenumber', 'Act', 'Scene', 'Play']:\n",
    "            new_row[col] = group[col].iloc[0]\n",
    "        new_row['combined_data'] = rows_combined\n",
    "        \n",
    "        return new_row\n",
    "\n",
    "    # Group and apply the combining function\n",
    "result = train_file1.groupby(group_cols, as_index=False).apply(combine_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['combined_data'].iloc[80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_col = result['combined_data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from transformers import GPT2Tokenizer, GPT2Config, GPT2Model, GPT2PreTrainedModel\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_col = text_col + ' ' + \"<|endoftext|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', pad_token='<|pad|>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2_Model(GPT2PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.transformer = GPT2Model.from_pretrained('gpt2')\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained('gpt2', pad_token='<|pad|>')\n",
    "\n",
    "        self.transformer.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        self.lm_head = nn.Linear(config.n_embd, len(tokenizer), bias=False)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        x = self.transformer(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)[0]\n",
    "        x = self.lm_head(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_data, tokenizer, gpt2_type=\"gpt2\", max_length=280):\n",
    "        self.texts = [tokenizer(data, truncation=True, max_length=max_length, padding=\"max_length\", return_tensors=\"pt\")\n",
    "                    for data in input_data]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLossFunction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CrossEntropyLossFunction, self).__init__()\n",
    "        self.loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, lm_logits, labels):\n",
    "        shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "        loss = self.loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, tokenizer, train_data, epochs, learning_rate, epsilon=1e-8):\n",
    "    train = CustomDataset(train_data, tokenizer)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=16, shuffle=True)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr = learning_rate, eps = epsilon)\n",
    "    criterion = CrossEntropyLossFunction().to(device)\n",
    "    model = model.to(device)\n",
    "\n",
    "    best_loss = 1000\n",
    "\n",
    "    for epoch_i in range(0, epochs):\n",
    "\n",
    "        total_train_loss = 0\n",
    "        total_val_loss = 0\n",
    "        for train_input in tqdm(train_dataloader):\n",
    "\n",
    "            mask = train_input['attention_mask'].to(device)\n",
    "            input_id = train_input['input_ids'].to(device)\n",
    "\n",
    "            outputs = model(input_id, attention_mask = mask, token_type_ids=None)\n",
    "\n",
    "            loss = criterion(outputs, input_id)\n",
    "\n",
    "            batch_loss = loss.item()\n",
    "            total_train_loss += batch_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "        print(f\"Epoch: {epoch_i}, Avg train loss: {np.round(avg_train_loss, 2)}\")\n",
    "\n",
    "\n",
    "epochs = 35\n",
    "learning_rate = 1e-5\n",
    "configuration = GPT2Config()\n",
    "gpt_model = GPT2_Model(configuration).to(device)\n",
    "\n",
    "train(gpt_model, tokenizer, text_col, epochs, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(idx, max_new_tokens, context_size, tokenizer, model, top_k=10, top_p=0.95):\n",
    "    for _ in range(max_new_tokens):\n",
    "        if idx[:,-1].item() != tokenizer.encode(tokenizer.eos_token)[0]:\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -context_size:]\n",
    "            # get the predictions\n",
    "            logits = model(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sort probabilities in descending order\n",
    "            sorted_probs, indices = torch.sort(probs, descending=True)\n",
    "            # compute cumsum of probabilities\n",
    "            probs_cumsum = torch.cumsum(sorted_probs, dim=1)\n",
    "            # choose only top_p tokens\n",
    "            sorted_probs, indices = sorted_probs[:, :probs_cumsum[[probs_cumsum < top_p]].size()[0] + 1], indices[:, :probs_cumsum[[probs_cumsum < top_p]].size()[0] +1]\n",
    "            # choose only top_k tokens\n",
    "            sorted_probs, indices = sorted_probs[:,:top_k], indices[:,:top_k]\n",
    "            # sample from the distribution\n",
    "            sorted_probs = F.softmax(sorted_probs, dim=-1)\n",
    "            idx_next = indices[:, torch.multinomial(sorted_probs, num_samples=1)].squeeze(0)\n",
    "            # append new token ids\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_model.eval()\n",
    "\n",
    "prompt = \"Generate a funny tweet.\"\n",
    "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "generated = generated.to(device)\n",
    "\n",
    "sample_outputs = generate(generated, max_new_tokens=200, context_size=256, tokenizer=tokenizer, model=gpt_model, top_k=10, top_p=0.95)\n",
    "\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(f\"{tokenizer.decode(sample_output, skip_special_tokens=True)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
